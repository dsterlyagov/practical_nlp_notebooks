{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pathlib\n",
    "import string\n",
    "from collections import Counter\n",
    "from natsort import natsorted\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir\n",
    "data_dir = []\n",
    "def read_data(dir_path):\n",
    "    # read all files based on given input dir path\n",
    "    # INPUT: dataset path\n",
    "    # OUTPUT: dataset\n",
    "    data = []\n",
    "    temp = []\n",
    "    # asign the path\n",
    "    source_dir = pathlib.Path(dir_path)\n",
    "    \n",
    "    # read the path\n",
    "    for file in source_dir.iterdir():\n",
    "        temp.append(file.name)\n",
    "#         print(\"Dataset found on: \", file)\n",
    "    \n",
    "    x = natsorted(temp)\n",
    "    for item in x:\n",
    "        y = str(dir_path) + '/' + str(item)\n",
    "        data_dir.append(y)\n",
    "    \n",
    "    # read the files\n",
    "    for item in data_dir:\n",
    "        print(item)\n",
    "        temp = open(item, \"r\", encoding=\"utf-8\")\n",
    "        row = temp.read()\n",
    "        data.append(row)\n",
    "        temp.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a1.txt', 'a10.txt', 'a2.txt', 'a3.txt', 'a4.txt', 'a5.txt', 'a6.txt', 'a7.txt', 'a8.txt', 'a9.txt']\n",
      "['a1.txt', 'a10.txt', 'a2.txt', 'a3.txt', 'a4.txt', 'a5.txt', 'a6.txt', 'a7.txt', 'a8.txt', 'a9.txt'] \n",
      " ['data_texts\\\\a1.txt', 'data_texts\\\\a10.txt', 'data_texts\\\\a2.txt', 'data_texts\\\\a3.txt', 'data_texts\\\\a4.txt', 'data_texts\\\\a5.txt', 'data_texts\\\\a6.txt', 'data_texts\\\\a7.txt', 'data_texts\\\\a8.txt', 'data_texts\\\\a9.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# used for looping through folders/files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def returnListOfFilePaths(folderPath):\n",
    "    fileInfo = []\n",
    "    listOfFileNames = [fileName for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    print(listOfFileNames)\n",
    "    listOfFilePaths = [join(folderPath, fileName) for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    fileInfo.append(listOfFileNames)\n",
    "    fileInfo.append(listOfFilePaths)\n",
    "    return fileInfo\n",
    "\n",
    "fileNames, filePaths = returnListOfFilePaths('data_texts')\n",
    "print(fileNames, \"\\n\", filePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_tokenization(dataset):\n",
    "    # подготовка к токенизации датасета\n",
    "    # на вход подается строка из датасета\n",
    "    # на выходе  список который привели к нижнему регистру\n",
    "    \n",
    "    # и удаляем пунктуацию\n",
    "    temp = dataset.lower()\n",
    "    punctuation_trimmer = temp.translate(str.maketrans('','',string.punctuation))\n",
    "    return(nltk.word_tokenize(punctuation_trimmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(txtFile):\n",
    "    # подготовительная часть токенизатора\n",
    "    # на вход массив со строками\n",
    "    # на выход токенизированный датасет лист\n",
    "    tok_data = []\n",
    "    for item in txtFile:\n",
    "        tok_data.append(grp_tokenization(item))\n",
    "    return tok_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_GrpCounter(tokens):\n",
    "    # счетчик токенов\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_counter(tokens):\n",
    "    # count the tokens in the token dataset\n",
    "    # INPUT: token array\n",
    "    # OUTPUT: counted token array\n",
    "    count = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        count.append(form_GrpCounter(item))\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_dataset(dataset):\n",
    "    # merge all files inside the given dataset\n",
    "    # INPUT: dataset array\n",
    "    # OUTPUT: dataset string\n",
    "    data = \"\"\n",
    "    for txt in dataset:\n",
    "        data = data + txt\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_texts/a1.txt\n",
      "./data_texts/a2.txt\n",
      "./data_texts/a3.txt\n",
      "./data_texts/a4.txt\n",
      "./data_texts/a5.txt\n",
      "./data_texts/a6.txt\n",
      "./data_texts/a7.txt\n",
      "./data_texts/a8.txt\n",
      "./data_texts/a9.txt\n",
      "./data_texts/a10.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./data_texts\"\n",
    "dataset = read_data(dataset_path)\n",
    "tokens = tokenization(dataset)\n",
    "count = form_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
