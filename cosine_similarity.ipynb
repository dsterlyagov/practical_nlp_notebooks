{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Jekill Hyde'\n",
    "url1 ='https://www.gutenberg.org/files/43/43-h/43-h.htm'\n",
    "\n",
    "# Treasure Island\n",
    "url2 = 'https://www.gutenberg.org/files/120/120-h/120-h.htm'\n",
    "\n",
    "# Dracula\n",
    "url3 = 'https://www.gutenberg.org/files/345/345-h/345-h.htm'\n",
    "\n",
    "#Two Cities\n",
    "url4 = 'https://www.gutenberg.org/files/98/98-h/98-h.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None ) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word_list(url):\n",
    "    req = requests.get(url)\n",
    "    html=req.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    text = soup.get_text()\n",
    "    tokenizer = LemNormalize\n",
    "    tokens = tokenizer(text)\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Jekill Hyde'\n",
    "clean1 = clean_word_list(url1)\n",
    "# Treasure Island\n",
    "clean2 = clean_word_list(url2)\n",
    "# Dracula\n",
    "clean3 = clean_word_list(url3)\n",
    "#Two Cities\n",
    "clean4 = clean_word_list(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(orig_list, separators=' '):\n",
    "    return separators.join(orig_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Jekill Hyde'\n",
    "string1 = list_to_string(clean1)\n",
    "string2 = list_to_string(clean2)\n",
    "string3 = list_to_string(clean3)\n",
    "string4 = list_to_string(clean4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lib = [string1, string2, string3, string4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfIdf_Vec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(textlist):\n",
    "    tfidf = TfIdf_Vec.fit_transform(textlist)\n",
    "    return (tfidf*tfidf.T).toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "stored = cosine_similarity(doc_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.78656176, 0.48735704, 0.82835259],\n",
       "       [0.78656176, 1.        , 0.4104891 , 0.84222084],\n",
       "       [0.48735704, 0.4104891 , 1.        , 0.40434309],\n",
       "       [0.82835259, 0.84222084, 0.40434309, 1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
